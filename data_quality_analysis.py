#!/usr/bin/env python3
"""
ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë„êµ¬
- ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨ ì´ë¯¸ì§€ ì°¾ê¸°
- íë¦¿í•œ ì´ë¯¸ì§€ ê°ì§€
- ì¤‘ë³µ ì´ë¯¸ì§€ íƒì§€
- ëª¨ë¸ ì‹ ë¢°ë„ ë‚®ì€ ì´ë¯¸ì§€ ë¶„ì„
- ì •ë¦¬ ì¶”ì²œì‚¬í•­ ì œê³µ
"""

import os
import cv2
import numpy as np
import pandas as pd
from glob import glob
import hashlib
from collections import defaultdict
import joblib
import json

class DataQualityAnalyzer:
    def __init__(self, data_dir="../faceshape-master/published_dataset"):
        self.data_dir = data_dir
        self.classes = ["heart", "oblong", "oval", "round", "square"]
        self.results = defaultdict(list)

    def analyze_all(self):
        """ì „ì²´ í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì‹œì‘...")

        # 1. ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        self.collect_basic_info()

        # 2. ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨ ë¶„ì„
        self.analyze_face_detection()

        # 3. ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„
        self.analyze_image_quality()

        # 4. ì¤‘ë³µ ì´ë¯¸ì§€ íƒì§€
        self.detect_duplicates()

        # 5. ML ëª¨ë¸ ì‹ ë¢°ë„ ë¶„ì„ (ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°)
        self.analyze_model_confidence()

        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report()

    def collect_basic_info(self):
        """ê¸°ë³¸ ë°ì´í„°ì…‹ ì •ë³´ ìˆ˜ì§‘"""
        print("ğŸ“Š ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

        total_images = 0
        class_counts = {}

        for cls in self.classes:
            folder_path = os.path.join(self.data_dir, cls)
            if os.path.exists(folder_path):
                images = glob(os.path.join(folder_path, "*"))
                images = [f for f in images if os.path.isfile(f)]
                class_counts[cls] = len(images)
                total_images += len(images)
            else:
                class_counts[cls] = 0

        self.results['basic_info'] = {
            'total_images': total_images,
            'class_counts': class_counts,
            'balanced': len(set(class_counts.values())) == 1
        }

        print(f"   ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
        print(f"   í´ë˜ìŠ¤ë³„: {class_counts}")

    def analyze_face_detection(self):
        """ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨ ì´ë¯¸ì§€ ë¶„ì„"""
        print("ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ë¶„ì„ ì¤‘...")

        cascade_path = cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
        face_cascade = cv2.CascadeClassifier(cascade_path)

        failed_detection = []
        multiple_faces = []

        for cls in self.classes:
            folder_path = os.path.join(self.data_dir, cls)
            if not os.path.exists(folder_path):
                continue

            images = glob(os.path.join(folder_path, "*"))

            for img_path in images:
                if not os.path.isfile(img_path):
                    continue

                img = cv2.imread(img_path)
                if img is None:
                    failed_detection.append({
                        'path': img_path,
                        'class': cls,
                        'reason': 'Cannot read image'
                    })
                    continue

                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))

                if len(faces) == 0:
                    failed_detection.append({
                        'path': img_path,
                        'class': cls,
                        'reason': 'No face detected'
                    })
                elif len(faces) > 1:
                    multiple_faces.append({
                        'path': img_path,
                        'class': cls,
                        'face_count': len(faces)
                    })

        self.results['face_detection'] = {
            'failed_detection': failed_detection,
            'multiple_faces': multiple_faces
        }

        print(f"   ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {len(failed_detection)}ê°œ")
        print(f"   ë‹¤ì¤‘ ì–¼êµ´: {len(multiple_faces)}ê°œ")

    def analyze_image_quality(self):
        """ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„ (ë¸”ëŸ¬, í¬ê¸°, ë°ê¸° ë“±)"""
        print("ğŸ–¼ï¸  ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„ ì¤‘...")

        low_quality = []
        size_issues = []

        for cls in self.classes:
            folder_path = os.path.join(self.data_dir, cls)
            if not os.path.exists(folder_path):
                continue

            images = glob(os.path.join(folder_path, "*"))

            for img_path in images:
                if not os.path.isfile(img_path):
                    continue

                img = cv2.imread(img_path)
                if img is None:
                    continue

                # 1. ë¸”ëŸ¬ ê²€ì¶œ (Laplacian variance)
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()

                # 2. ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
                h, w = img.shape[:2]

                # 3. ë°ê¸° ë¶„ì„
                brightness = np.mean(gray)

                # í’ˆì§ˆ í‰ê°€
                if blur_score < 100:  # ë¸”ëŸ¬ ì„ê³„ê°’
                    low_quality.append({
                        'path': img_path,
                        'class': cls,
                        'reason': 'Too blurry',
                        'blur_score': blur_score
                    })

                if w < 100 or h < 100:  # í¬ê¸° ì„ê³„ê°’
                    size_issues.append({
                        'path': img_path,
                        'class': cls,
                        'size': (w, h),
                        'reason': 'Too small'
                    })

                if brightness < 30 or brightness > 225:  # ë°ê¸° ì„ê³„ê°’
                    low_quality.append({
                        'path': img_path,
                        'class': cls,
                        'reason': 'Poor lighting',
                        'brightness': brightness
                    })

        self.results['image_quality'] = {
            'low_quality': low_quality,
            'size_issues': size_issues
        }

        print(f"   í’ˆì§ˆ ë¬¸ì œ: {len(low_quality)}ê°œ")
        print(f"   í¬ê¸° ë¬¸ì œ: {len(size_issues)}ê°œ")

    def detect_duplicates(self):
        """ì¤‘ë³µ ì´ë¯¸ì§€ íƒì§€ (í•´ì‹œ ê¸°ë°˜)"""
        print("ğŸ”„ ì¤‘ë³µ ì´ë¯¸ì§€ íƒì§€ ì¤‘...")

        hash_dict = defaultdict(list)

        for cls in self.classes:
            folder_path = os.path.join(self.data_dir, cls)
            if not os.path.exists(folder_path):
                continue

            images = glob(os.path.join(folder_path, "*"))

            for img_path in images:
                if not os.path.isfile(img_path):
                    continue

                try:
                    with open(img_path, 'rb') as f:
                        img_hash = hashlib.md5(f.read()).hexdigest()
                        hash_dict[img_hash].append({
                            'path': img_path,
                            'class': cls
                        })
                except:
                    continue

        # ì¤‘ë³µ ì°¾ê¸°
        duplicates = []
        for img_hash, img_list in hash_dict.items():
            if len(img_list) > 1:
                duplicates.append(img_list)

        self.results['duplicates'] = duplicates
        print(f"   ì¤‘ë³µ ê·¸ë£¹: {len(duplicates)}ê°œ")

    def analyze_model_confidence(self):
        """ML ëª¨ë¸ ì‹ ë¢°ë„ ë¶„ì„"""
        print("ğŸ¤– ëª¨ë¸ ì‹ ë¢°ë„ ë¶„ì„ ì¤‘...")

        model_path = "face_shape_rf_model.pkl"
        label_path = "label_encoder.pkl"

        if not os.path.exists(model_path) or not os.path.exists(label_path):
            print("   ML ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            self.results['model_confidence'] = {'analyzed': False}
            return

        try:
            # ëª¨ë¸ ë¡œë“œ
            clf = joblib.load(model_path)
            le = joblib.load(label_path)

            low_confidence = []
            wrong_predictions = []

            for cls in self.classes:
                folder_path = os.path.join(self.data_dir, cls)
                if not os.path.exists(folder_path):
                    continue

                images = glob(os.path.join(folder_path, "*"))

                for img_path in images[:10]:  # ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì ˆì•½)
                    if not os.path.isfile(img_path):
                        continue

                    try:
                        # ì´ë¯¸ì§€ ë¡œë“œ ë° íŠ¹ì§• ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)
                        img = cv2.imread(img_path)
                        if img is None:
                            continue

                        # ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œ (ì‹¤ì œ HOG+Color ëŒ€ì‹ )
                        img_resized = cv2.resize(img, (64, 64))
                        features = img_resized.flatten().astype(np.float32)

                        # ì°¨ì› ë§ì¶”ê¸° (ì‹¤ì œ ëª¨ë¸ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                        if features.shape[0] != 8612:  # ì‹¤ì œ íŠ¹ì§• ì°¨ì›
                            continue

                        # ì˜ˆì¸¡
                        features = features.reshape(1, -1)
                        pred_idx = clf.predict(features)[0]
                        prob = clf.predict_proba(features)[0]

                        predicted_class = le.inverse_transform([pred_idx])[0]
                        confidence = np.max(prob)

                        # ë¶„ì„
                        if confidence < 0.5:  # ë‚®ì€ ì‹ ë¢°ë„
                            low_confidence.append({
                                'path': img_path,
                                'true_class': cls.capitalize(),
                                'predicted_class': predicted_class,
                                'confidence': confidence
                            })

                        if predicted_class.lower() != cls:  # ì˜ëª»ëœ ì˜ˆì¸¡
                            wrong_predictions.append({
                                'path': img_path,
                                'true_class': cls.capitalize(),
                                'predicted_class': predicted_class,
                                'confidence': confidence
                            })

                    except Exception as e:
                        continue

            self.results['model_confidence'] = {
                'analyzed': True,
                'low_confidence': low_confidence,
                'wrong_predictions': wrong_predictions
            }

            print(f"   ë‚®ì€ ì‹ ë¢°ë„: {len(low_confidence)}ê°œ")
            print(f"   ì˜ëª»ëœ ì˜ˆì¸¡: {len(wrong_predictions)}ê°œ")

        except Exception as e:
            print(f"   ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.results['model_confidence'] = {'analyzed': False}

    def generate_report(self):
        """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("="*60)

        # ê¸°ë³¸ ì •ë³´
        basic = self.results['basic_info']
        print(f"\nğŸ“Š ê¸°ë³¸ ì •ë³´:")
        print(f"   ì´ ì´ë¯¸ì§€: {basic['total_images']}ê°œ")
        print(f"   í´ë˜ìŠ¤ë³„ ë¶„í¬: {basic['class_counts']}")
        print(f"   ê· í˜• ì—¬ë¶€: {'âœ… ê· í˜•' if basic['balanced'] else 'âŒ ë¶ˆê· í˜•'}")

        # ë¬¸ì œ ìš”ì•½
        face_issues = len(self.results['face_detection']['failed_detection'])
        quality_issues = len(self.results['image_quality']['low_quality'])
        duplicate_groups = len(self.results['duplicates'])

        total_problems = face_issues + quality_issues + sum(len(group) for group in self.results['duplicates'])

        print(f"\nğŸš¨ ë°œê²¬ëœ ë¬¸ì œ:")
        print(f"   ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {face_issues}ê°œ")
        print(f"   í’ˆì§ˆ ë¬¸ì œ: {quality_issues}ê°œ")
        print(f"   ì¤‘ë³µ ì´ë¯¸ì§€: {duplicate_groups}ê°œ ê·¸ë£¹")
        print(f"   ì´ ë¬¸ì œ ì´ë¯¸ì§€: {total_problems}ê°œ")

        # ê°œì„  ì œì•ˆ
        print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        improvement_potential = (total_problems / basic['total_images']) * 100 if basic['total_images'] > 0 else 0
        print(f"   ë¬¸ì œ ì´ë¯¸ì§€ ì œê±°ì‹œ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: +{improvement_potential:.1f}%")

        if face_issues > 0:
            print(f"   - {face_issues}ê°œ ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨ ì´ë¯¸ì§€ ì œê±° ê¶Œì¥")
        if quality_issues > 0:
            print(f"   - {quality_issues}ê°œ ì €í’ˆì§ˆ ì´ë¯¸ì§€ ì¬ê²€í†  ê¶Œì¥")
        if duplicate_groups > 0:
            print(f"   - {duplicate_groups}ê°œ ì¤‘ë³µ ê·¸ë£¹ì—ì„œ ì¤‘ë³µ ì œê±° ê¶Œì¥")

        # ìƒì„¸ ë¬¸ì œ ëª©ë¡ ì €ì¥
        self.save_detailed_report()

        print(f"\nğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ê°€ 'data_quality_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("="*60)

    def save_detailed_report(self):
        """ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open('data_quality_report.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)

    def get_cleanup_recommendations(self):
        """ì •ë¦¬ ì¶”ì²œì‚¬í•­ ë°˜í™˜"""
        recommendations = []

        # ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨ â†’ ì‚­ì œ ì¶”ì²œ
        for item in self.results['face_detection']['failed_detection']:
            recommendations.append({
                'action': 'delete',
                'path': item['path'],
                'reason': f"ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {item['reason']}"
            })

        # ì €í’ˆì§ˆ ì´ë¯¸ì§€ â†’ ì¬ê²€í†  ì¶”ì²œ
        for item in self.results['image_quality']['low_quality']:
            recommendations.append({
                'action': 'review',
                'path': item['path'],
                'reason': f"í’ˆì§ˆ ë¬¸ì œ: {item['reason']}"
            })

        # ì¤‘ë³µ ì´ë¯¸ì§€ â†’ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ì‚­ì œ
        for group in self.results['duplicates']:
            for i, item in enumerate(group[1:], 1):  # ì²« ë²ˆì§¸ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
                recommendations.append({
                    'action': 'delete',
                    'path': item['path'],
                    'reason': f"ì¤‘ë³µ ì´ë¯¸ì§€ (ê·¸ë£¹ {i+1})"
                })

        return recommendations

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = DataQualityAnalyzer()
    analyzer.analyze_all()

    # ì •ë¦¬ ì¶”ì²œì‚¬í•­
    recommendations = analyzer.get_cleanup_recommendations()

    print(f"\nğŸ§¹ ì •ë¦¬ ì¶”ì²œì‚¬í•­: {len(recommendations)}ê°œ")

    # ì²˜ë¦¬ ì—¬ë¶€ ë¬»ê¸°
    if recommendations:
        print("\nì²˜ë¦¬í•  ì‘ì—…:")
        delete_count = len([r for r in recommendations if r['action'] == 'delete'])
        review_count = len([r for r in recommendations if r['action'] == 'review'])

        print(f"   ì‚­ì œ ì¶”ì²œ: {delete_count}ê°œ")
        print(f"   ì¬ê²€í†  ì¶”ì²œ: {review_count}ê°œ")

        print("\nâš ï¸  ì‹¤ì œ íŒŒì¼ ì‚­ì œëŠ” ìˆ˜ë™ìœ¼ë¡œ í™•ì¸ í›„ ì§„í–‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")

if __name__ == "__main__":
    main()